{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d44d3fb0",
   "metadata": {},
   "source": [
    "# Prueba T√©cnica ‚Äì Ingeniero/a de IA Aplicada: Sistemas RAG y LLMs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04c04b1",
   "metadata": {},
   "source": [
    "## Importar librer√≠as necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dc02f605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b0777f",
   "metadata": {},
   "source": [
    "## 1. Data Ingestion "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f990a650",
   "metadata": {},
   "source": [
    "- Selecciona una base documental (puede ser p√∫blica; se sugiere documentaci√≥n t√©cnica o art√≠culos en PDF/HTML/TXT). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "19843d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_url(url):\n",
    "    \"\"\"\n",
    "    Descarga una p√°gina web en formato HTML y extrae su contenido textual limpio.\n",
    "\n",
    "    Este m√©todo realiza una solicitud HTTP a la URL proporcionada, elimina elementos\n",
    "    no textuales del HTML (como scripts, estilos, navegaci√≥n, etc.), y retorna el\n",
    "    texto visible, limpio y estructurado en l√≠neas.\n",
    "\n",
    "    Args:\n",
    "        url (str): URL de la p√°gina web a procesar.\n",
    "\n",
    "    Returns:\n",
    "        str: Texto limpio extra√≠do del contenido HTML de la p√°gina.\n",
    "\n",
    "    Raises:\n",
    "        Exception: Si la solicitud HTTP devuelve un c√≥digo diferente a 200.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Error al descargar la p√°gina: {response.status_code}\")\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Eliminamos scripts, estilos, navegaci√≥n, etc.\n",
    "    for tag in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\", \"aside\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    # Extraemos el texto visible y limpiamos espacios\n",
    "    text = soup.get_text(separator=\"\\n\")\n",
    "    lines = [line.strip() for line in text.splitlines()]\n",
    "    clean_text = \"\\n\".join(line for line in lines if line)\n",
    "\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "01c1d890",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_texts(urls):\n",
    "    \"\"\"\n",
    "    Extrae y concatena el texto limpio de m√∫ltiples URLs en formato HTML.\n",
    "\n",
    "    Para cada URL en la lista, se descarga el contenido HTML, se limpia y se\n",
    "    extrae el texto visible utilizando la funci√≥n `extract_text_from_url`.\n",
    "    Si ocurre un error durante la descarga o procesamiento, se imprime el\n",
    "    error y se contin√∫a con las siguientes URLs.\n",
    "\n",
    "    Args:\n",
    "        urls (list of str): Lista de URLs a procesar.\n",
    "\n",
    "    Returns:\n",
    "        str: Texto combinado de todas las p√°ginas, separado por saltos de l√≠nea.\n",
    "    \"\"\"\n",
    "    all_text = \"\"\n",
    "    for url in urls:\n",
    "        try:\n",
    "            print(f\"üîó Procesando: {url}\")\n",
    "            all_text += extract_text_from_url(url) + \"\\n\"\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error en {url}: {e}\")\n",
    "    return all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3f087f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Procesando: https://python.langchain.com/docs/versions/v0_3/\n",
      "üîó Procesando: https://python.langchain.com/docs/introduction/\n",
      "üîó Procesando: https://python.langchain.com/docs/tutorials/\n",
      "üîó Procesando: https://python.langchain.com/docs/tutorials/retrievers/\n",
      "üîó Procesando: https://python.langchain.com/docs/concepts/document_loaders/\n",
      "üîó Procesando: https://python.langchain.com/docs/concepts/embedding_models/\n",
      "üîó Procesando: https://python.langchain.com/docs/concepts/vectorstores/\n",
      "üîó Procesando: https://python.langchain.com/docs/tutorials/classification/\n",
      "üîó Procesando: https://python.langchain.com/docs/concepts/structured_outputs/\n",
      "üîó Procesando: https://python.langchain.com/docs/tutorials/extraction/\n",
      "üîó Procesando: https://python.langchain.com/docs/how_to/\n",
      "üîó Procesando: https://python.langchain.com/docs/how_to/structured_output/\n",
      "üîó Procesando: https://python.langchain.com/docs/how_to/tool_calling/\n",
      "üîó Procesando: https://python.langchain.com/docs/how_to/streaming/\n",
      "üîó Procesando: https://python.langchain.com/docs/how_to/debugging/\n",
      "üîó Procesando: https://python.langchain.com/docs/concepts/few_shot_prompting/\n",
      "üîó Procesando: https://python.langchain.com/docs/concepts/chat_models/\n",
      "LangChain v0.3 | ü¶úÔ∏èüîó LangChain\n",
      "Skip to main content\n",
      "Our\n",
      "Building Ambient Agents with LangGraph\n",
      "course is now available on LangChain Academy!\n",
      "On this page\n",
      "Last updated: 09.16.24\n",
      "What's changed\n",
      "‚Äã\n",
      "All packages have been upgraded from Pydantic 1 to Pydantic 2 internally. Use of Pydantic 2 in user code is fully supported with all packages without the need for bridges like\n",
      "langchain_core.pydantic_v1\n",
      "or\n",
      "pydantic.v1\n",
      ".\n",
      "Pydantic 1 will no longer be supported as it reached its end-of-life in June 2024.\n",
      "Python 3.8 will no longer be supported as its end-of-life is October 2024.\n",
      "These are the only breaking changes.\n",
      "What‚Äôs new\n",
      "‚Äã\n",
      "The following features have been added during the development of 0.2.x:\n",
      "Moved more integrations from\n",
      "langchain-community\n",
      "to their own\n",
      "langchain-x\n",
      "packages. This is a non-breaking change, as the legacy implementations are left in\n",
      "langchain-community\n",
      "and marked as deprecated. This allows us to better manage the dependencies of, test, and version these integrations. You can see \n"
     ]
    }
   ],
   "source": [
    "# Links de LangChain\n",
    "urls = [\n",
    "    \"https://python.langchain.com/docs/versions/v0_3/\",\n",
    "    \"https://python.langchain.com/docs/introduction/\",\n",
    "    \"https://python.langchain.com/docs/tutorials/\",\n",
    "    \"https://python.langchain.com/docs/tutorials/retrievers/\",\n",
    "    \"https://python.langchain.com/docs/concepts/document_loaders/\",\n",
    "    \"https://python.langchain.com/docs/concepts/embedding_models/\",\n",
    "    \"https://python.langchain.com/docs/concepts/vectorstores/\",\n",
    "    \"https://python.langchain.com/docs/tutorials/classification/\",\n",
    "    \"https://python.langchain.com/docs/concepts/structured_outputs/\", \n",
    "    \"https://python.langchain.com/docs/tutorials/extraction/\", \n",
    "    \"https://python.langchain.com/docs/how_to/\", \n",
    "    \"https://python.langchain.com/docs/how_to/structured_output/\",\n",
    "    \"https://python.langchain.com/docs/how_to/tool_calling/\", \n",
    "    \"https://python.langchain.com/docs/how_to/streaming/\", \n",
    "    \"https://python.langchain.com/docs/how_to/debugging/\", \n",
    "    \"https://python.langchain.com/docs/concepts/few_shot_prompting/\", \n",
    "    \"https://python.langchain.com/docs/concepts/chat_models/\",\n",
    "]\n",
    "\n",
    "raw_text = extract_all_texts(urls)\n",
    "print(raw_text[:1000]) # Muestra los primeros 1000 caracteres del texto extra√≠do"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ae216f",
   "metadata": {},
   "source": [
    "- Procesa e indexa la informaci√≥n para facilitar la recuperaci√≥n sem√°ntica. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d883f0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 555, which is longer than the specified 500\n",
      "Created a chunk of size 1000, which is longer than the specified 500\n",
      "Created a chunk of size 555, which is longer than the specified 500\n",
      "Created a chunk of size 1000, which is longer than the specified 500\n",
      "Created a chunk of size 877, which is longer than the specified 500\n",
      "Created a chunk of size 590, which is longer than the specified 500\n",
      "Created a chunk of size 627, which is longer than the specified 500\n",
      "Created a chunk of size 680, which is longer than the specified 500\n",
      "Created a chunk of size 589, which is longer than the specified 500\n",
      "Created a chunk of size 616, which is longer than the specified 500\n",
      "Created a chunk of size 693, which is longer than the specified 500\n",
      "Created a chunk of size 636, which is longer than the specified 500\n",
      "Created a chunk of size 510, which is longer than the specified 500\n",
      "Created a chunk of size 506, which is longer than the specified 500\n",
      "Created a chunk of size 930, which is longer than the specified 500\n",
      "Created a chunk of size 7630, which is longer than the specified 500\n",
      "Created a chunk of size 7381, which is longer than the specified 500\n",
      "Created a chunk of size 5421, which is longer than the specified 500\n",
      "Created a chunk of size 2152, which is longer than the specified 500\n",
      "Created a chunk of size 2811, which is longer than the specified 500\n",
      "Created a chunk of size 7602, which is longer than the specified 500\n",
      "Created a chunk of size 4945, which is longer than the specified 500\n",
      "Created a chunk of size 12870, which is longer than the specified 500\n",
      "Created a chunk of size 554, which is longer than the specified 500\n",
      "Created a chunk of size 557, which is longer than the specified 500\n",
      "Created a chunk of size 555, which is longer than the specified 500\n",
      "Created a chunk of size 556, which is longer than the specified 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de chunks generados: 482\n",
      "LangChain v0.3 | ü¶úÔ∏èüîó LangChain\n",
      "Skip to main content\n",
      "Our\n",
      "Building Ambient Agents with LangGraph\n",
      "course is now available on LangChain Academy!\n",
      "On this page\n",
      "Last updated: 09.16.24\n",
      "What's changed\n",
      "‚Äã\n",
      "All packages have been upgraded from Pydantic 1 to Pydantic 2 internally. Use of Pydantic 2 in user code is fully supported with all packages without the need for bridges like\n",
      "langchain_core.pydantic_v1\n",
      "or\n",
      "pydantic.v1\n",
      ".\n",
      "Pydantic 1 will no longer be supported as it reached its end-of-life in June 2024.\n"
     ]
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(raw_text)\n",
    "\n",
    "print(f\"Total de chunks generados: {len(chunks)}\")\n",
    "print(chunks[0])  # Muestra el primer chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743f190e",
   "metadata": {},
   "source": [
    "## 2. Retrieval Pipeline (RAG) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb5e94f",
   "metadata": {},
   "source": [
    "- Implementa un motor de b√∫squeda sem√°ntica (p. ej., con FAISS o Elasticsearch). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "31551826",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:11<00:00,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (482, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Modelo compacto y eficiente\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Crea los embeddings\n",
    "embeddings = embedding_model.encode(chunks, show_progress_bar=True)\n",
    "\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "20a0c602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de vectores indexados: 482\n"
     ]
    }
   ],
   "source": [
    "# Creamos el √≠ndice FAISS\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(np.array(embeddings))\n",
    "\n",
    "# Asociamos los chunks con sus √≠ndices (opcional, √∫til para trazabilidad)\n",
    "chunk_map = {i: chunk for i, chunk in enumerate(chunks)}\n",
    "\n",
    "print(f\"Total de vectores indexados: {index.ntotal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a73722e",
   "metadata": {},
   "source": [
    "- Realiza  embedding  de  los  documentos  usando  un  modelo  como  sentence-\n",
    "transformers o equivalente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9e6bff12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_similar_chunks(question, top_k=3):\n",
    "    \"\"\"\n",
    "    Recupera los chunks m√°s relevantes del corpus en funci√≥n de una pregunta dada.\n",
    "\n",
    "    Genera el embedding de la pregunta, lo compara con los vectores indexados en FAISS\n",
    "    y devuelve los `top_k` chunks m√°s similares. Los resultados se concatenan como un solo\n",
    "    string separado por doble salto de l√≠nea.\n",
    "\n",
    "    Args:\n",
    "        question (str): Pregunta en lenguaje natural.\n",
    "        top_k (int, optional): N√∫mero de chunks m√°s relevantes a recuperar. Por defecto es 3.\n",
    "\n",
    "    Returns:\n",
    "        str: Texto combinado de los chunks m√°s similares, separados por saltos de l√≠nea dobles.\n",
    "    \"\"\"\n",
    "    query_embedding = embedding_model.encode([question])\n",
    "    distances, indices = index.search(np.array(query_embedding), top_k)\n",
    "    retrieved_chunks = [chunk_map[i] for i in indices[0]]\n",
    "    return \"\\n\\n\".join(retrieved_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "79f7d0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Contexto recuperado:\n",
      "\n",
      "this page\n",
      ".\n",
      "Integrations\n",
      "‚Äã\n",
      "LangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\n",
      "If you're looking to get up and running quickly with\n",
      "chat models\n",
      ",\n",
      "vector stores\n",
      ",\n",
      "or other LangChain components from a specific provider, check out our growing list of\n",
      "integrations\n",
      ".\n",
      "API reference\n",
      "‚Äã\n",
      "Head to the reference section for full documentation of all classes and methods in the LangChain Python packages.\n",
      "Ecosystem\n",
      "‚Äã\n",
      "ü¶úüõ†Ô∏è LangSmith\n",
      "‚Äã\n",
      "\n",
      "here\n",
      ".\n",
      "Marked as deprecated a number of legacy chains and added migration guides for all of them. These are slated for removal in\n",
      "langchain\n",
      "1.0.0. See the deprecated chains and associated\n",
      "migration guides here\n",
      ".\n",
      "How to update your code\n",
      "‚Äã\n",
      "If you're using\n",
      "langchain\n",
      "/\n",
      "langchain-community\n",
      "/\n",
      "langchain-core\n",
      "0.0 or 0.1, we recommend that you first\n",
      "upgrade to 0.2\n",
      ".\n",
      "If you're using\n",
      "langgraph\n",
      ", upgrade to\n",
      "langgraph>=0.2.20,<0.3\n",
      ". This will work with either 0.2 or 0.3 versions of all the base packages.\n",
      "\n",
      "Introduction | ü¶úÔ∏èüîó LangChain\n",
      "Skip to main content\n",
      "Our\n",
      "Building Ambient Agents with LangGraph\n",
      "course is now available on LangChain Academy!\n",
      "On this page\n",
      "LangChain\n",
      "is a framework for developing applications powered by large language models (LLMs).\n",
      "LangChain simplifies every stage of the LLM application lifecycle:\n",
      "Development\n",
      ": Build your applications using LangChain's open-source\n",
      "components\n",
      "and\n",
      "third-party integrations\n",
      ".\n",
      "Use\n",
      "LangGraph\n",
      "\n",
      "Ecosystem\n",
      "ü¶úüõ†Ô∏è LangSmith\n",
      "ü¶úüï∏Ô∏è LangGraph\n",
      "Additional resources\n",
      "Versions\n",
      "Security\n",
      "Contributing\n",
      "Tutorials | ü¶úÔ∏èüîó LangChain\n",
      "Skip to main content\n",
      "Our\n",
      "Building Ambient Agents with LangGraph\n",
      "course is now available on LangChain Academy!\n",
      "On this page\n",
      "New to LangChain or LLM app development in general? Read this material to quickly get up and running building your first applications.\n",
      "Get started\n",
      "‚Äã\n",
      "Familiarize yourself with LangChain's open-source components by building simple applications.\n",
      "\n",
      "LangSmith Tracing: This logs events to\n",
      "LangSmith\n",
      "to allow for visualization there.\n",
      "Verbose Mode\n",
      "Debug Mode\n",
      "LangSmith Tracing\n",
      "Free\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "UI\n",
      "‚ùå\n",
      "‚ùå\n",
      "‚úÖ\n",
      "Persisted\n",
      "‚ùå\n",
      "‚ùå\n",
      "‚úÖ\n",
      "See all events\n",
      "‚ùå\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "See \"important\" events\n",
      "‚úÖ\n",
      "‚ùå\n",
      "‚úÖ\n",
      "Runs Locally\n",
      "‚úÖ\n",
      "‚úÖ\n",
      "‚ùå\n",
      "Tracing\n",
      "‚Äã\n",
      "Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\n",
      "As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is LangChain and what is it used for?\"\n",
    "context = search_similar_chunks(question, top_k=5)\n",
    "print(\"üîç Contexto recuperado:\\n\")\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465adc45",
   "metadata": {},
   "source": [
    "## 3. Generaci√≥n con LLM "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7258fd75",
   "metadata": {},
   "source": [
    "- Integra un modelo de lenguaje como OpenAI, HuggingFace, o Llama.cpp.. \n",
    "- Aseg√∫rate de que las respuestas generadas est√©n condicionadas por los documentos recuperados (prompting controlado). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0046203d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google/flan-t5-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Carga un pipeline de generaci√≥n\n",
    "llm_pipeline = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1cff6fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(context, question):\n",
    "    \"\"\"\n",
    "    Construye un prompt instructivo para un modelo de lenguaje, usando contexto y pregunta.\n",
    "\n",
    "    El prompt resultante indica expl√≠citamente al modelo que debe generar una respuesta\n",
    "    basada √∫nicamente en el contexto proporcionado, lo cual es esencial en sistemas RAG\n",
    "    para reducir la alucinaci√≥n y aumentar la fidelidad de la respuesta.\n",
    "\n",
    "    Args:\n",
    "        context (str): Fragmento(s) de texto recuperado(s) desde el corpus (chunks relevantes).\n",
    "        question (str): Pregunta formulada por el usuario.\n",
    "\n",
    "    Returns:\n",
    "        str: Prompt formateado para ser procesado por un modelo generativo.\n",
    "    \"\"\"\n",
    "    return f\"\"\"Answer the following question based solely on the context provided.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "72c9f14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question):\n",
    "    \"\"\"\n",
    "    Genera una respuesta en lenguaje natural utilizando un modelo LLM y recuperaci√≥n sem√°ntica.\n",
    "\n",
    "    Esta funci√≥n realiza los siguientes pasos:\n",
    "    1. Recupera los chunks m√°s relevantes desde el √≠ndice sem√°ntico basado en la pregunta.\n",
    "    2. Construye un prompt controlado con dicho contexto y la pregunta original.\n",
    "    3. Env√≠a el prompt al modelo LLM para generar una respuesta.\n",
    "\n",
    "    Args:\n",
    "        question (str): Pregunta en lenguaje natural a responder.\n",
    "\n",
    "    Returns:\n",
    "        str: Respuesta generada por el modelo de lenguaje, basada en el contexto recuperado.\n",
    "    \"\"\"\n",
    "    context = search_similar_chunks(question, top_k=3)\n",
    "    prompt = build_prompt(context, question)\n",
    "    response = llm_pipeline(prompt, max_new_tokens=512, do_sample=False)[0][\"generated_text\"]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5656d5",
   "metadata": {},
   "source": [
    "## 4. Evaluaci√≥n b√°sica "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0546fa",
   "metadata": {},
   "source": [
    "- Proporciona ejemplos de preguntas sobre los documentos. \n",
    "- Incluye  m√©tricas  o criterios para verificar que  las respuestas  generadas sean \n",
    "relevantes y fieles al contenido. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "045aaf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "preguntas = [\n",
    "    \"What is LangChain?\",\n",
    "    \"What are the main components of LangChain?\",\n",
    "    \"What are LLM models used for in LangChain?\",\n",
    "    \"Can LangChain be integrated with databases?\",\n",
    "    \"What are the advantages of using LangChain in production?\",\n",
    "    \"How does LangChain differ from direct use of OpenAI API?\",\n",
    "    \"What are agents in LangChain and how do they work?\",\n",
    "    \"Can LangChain interact with external APIs?\",\n",
    "    \"How does LangChain support memory in conversations?\",\n",
    "    \"What are tools in LangChain and how are they defined?\",\n",
    "    \"How can I integrate LangChain with vector databases like FAISS or Pinecone?\",\n",
    "    \"What types of memory does LangChain support?\",\n",
    "    \"How do you persist memory in a LangChain app?\",\n",
    "    \"What are retrieval-based QA chains in LangChain?\",\n",
    "    \"How does LangChain handle prompt templating?\",\n",
    "    \"What is the role of LangChain Expression Language (LCEL)?\",\n",
    "    \"How do you debug chains and agents in LangChain?\",\n",
    "    \"Can LangChain be used with Hugging Face models?\",\n",
    "    \"How does LangChain handle streaming responses?\",\n",
    "    \"What are some production-ready deployment strategies for LangChain apps?\",\n",
    "    \"How can LangChain be integrated with frameworks like FastAPI or Streamlit?\",\n",
    "    \"What are common use cases of LangChain in enterprise settings?\",\n",
    "    \"How does LangChain work with multi-modal models?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f8da9e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Pregunta: What is LangChain?\n",
      "üí¨ Respuesta: a framework for developing applications powered by large language models\n",
      "--------------------------------------------------------------------------------\n",
      "üß† Pregunta: What are the main components of LangChain?\n",
      "üí¨ Respuesta: chat models, vector stores, and other LangChain components\n",
      "--------------------------------------------------------------------------------\n",
      "üß† Pregunta: What are LLM models used for in LangChain?\n",
      "üí¨ Respuesta: building robust and stateful multi-actor applications\n",
      "--------------------------------------------------------------------------------\n",
      "üß† Pregunta: Can LangChain be integrated with databases?\n",
      "üí¨ Respuesta: Yes\n",
      "--------------------------------------------------------------------------------\n",
      "üß† Pregunta: What are the advantages of using LangChain in production?\n",
      "üí¨ Respuesta: LangGraph builds stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by Linkedin, Uber, Klarna, GitLab, and many more.\n",
      "--------------------------------------------------------------------------------\n",
      "üß† Pregunta: How does LangChain differ from direct use of OpenAI API?\n",
      "üí¨ Respuesta: LangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it. If you're looking to get up and running quickly with chat models , vector stores , or other LangChain components from a specific provider, check out our growing list of integrations . API reference Head to the reference section for full documentation of all classes and methods in the LangChain Python packages. Ecosystem  LangSmith LangChain Message Format : LangChain's own message format, which is used by default and is used internally by LangChain. OpenAI's Message Format : OpenAI's message format. Standard parameters Many chat models have standardized parameters that can be used to configure the model: Parameter Description model The name or identifier of the specific AI model you want to use (e.g., \"gpt-3.5-turbo\" or \"gpt-4\" ). temperature\n",
      "--------------------------------------------------------------------------------\n",
      "üß† Pregunta: What are agents in LangChain and how do they work?\n",
      "üí¨ Respuesta: stateful agents with first-class streaming and human-in-the-loop support\n",
      "--------------------------------------------------------------------------------\n",
      "üß† Pregunta: Can LangChain interact with external APIs?\n",
      "üí¨ Respuesta: Yes\n",
      "--------------------------------------------------------------------------------\n",
      "üß† Pregunta: How does LangChain support memory in conversations?\n",
      "üí¨ Respuesta: Chatbots : Build a chatbot that incorporates memory\n",
      "--------------------------------------------------------------------------------\n",
      "üß† Pregunta: What are tools in LangChain and how are they defined?\n",
      "üí¨ Respuesta: LangChain Tools contain a description of the tool (to pass to the language model) as well as the implementation of the function to call. Refer here for a list of pre-built tools.\n",
      "--------------------------------------------------------------------------------\n",
      "üß† Pregunta: How can I integrate LangChain with vector databases like FAISS or Pinecone?\n",
      "üí¨ Respuesta: LangChain has a large number of vectorstore integrations, allowing users to easily switch between different vectorstore implementations. Please see the full list of LangChain vectorstore integrations . API reference Head to the reference section for full documentation of all classes and methods in the LangChain Python packages. Ecosystem  LangSmith Tutorials and the API Reference . However, these guides will help you quickly accomplish common tasks using chat models , vector stores , and other common LangChain components. Check out LangGraph-specific how-tos here . Conceptual guide Introductions to all the key parts of LangChain you‚Äôll need to know! Here you'll find high level explanations of all LangChain concepts. For a deeper dive into LangGraph concepts, check out this page .\n",
      "--------------------------------------------------------------------------------\n",
      "üß† Pregunta: What types of memory does LangChain support?\n",
      "üí¨ Respuesta: unanswerable\n",
      "--------------------------------------------------------------------------------\n",
      "üß† Pregunta: How do you persist memory in a LangChain app?\n",
      "üí¨ Respuesta: pass through inputs from one chain step to the next\n",
      "--------------------------------------------------------------------------------\n",
      "üß† Pregunta: What are retrieval-based QA chains in LangChain?\n",
      "üí¨ Respuesta: Retrieval Augmented Generation (RAG) Part 1 Serialization How to: save and load LangChain objects Use cases These guides cover use-case specific details. Q&A with RAG Retrieval Augmented Generation (RAG) is a way to connect LLMs to external sources of data. For a high-level tutorial on RAG, check out this guide . How to: add chat history How to: stream How to: return sources How to: return citations How to: do per-user retrieval Extraction Extraction is when you use LLMs to extract structured information from unstructured text. This page . Integrations LangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it. If you're looking to get up and running quickly with chat models , vector stores , or other LangChain components from a specific provider, check out our growing list of integrations . API reference Head to the reference section for full documentation of all classes and methods in the LangChain Python packages.\n",
      "--------------------------------------------------------------------------------\n",
      "üß† Pregunta: How does LangChain handle prompt templating?\n",
      "üí¨ Respuesta: import ChatPromptTemplate\n",
      "--------------------------------------------------------------------------------\n",
      "üß† Pregunta: What is the role of LangChain Expression Language (LCEL)?\n",
      "üí¨ Respuesta: create arbitrary custom chains\n",
      "--------------------------------------------------------------------------------\n",
      "üß† Pregunta: How do you debug chains and agents in LangChain?\n",
      "üí¨ Respuesta: set_debug(True) Setting the global debug flag will cause all LangChain components with callback support (chains, models, agents, tools, retrievers) to print the inputs they receive and outputs they generate.\n",
      "--------------------------------------------------------------------------------\n",
      "üß† Pregunta: Can LangChain be used with Hugging Face models?\n",
      "üí¨ Respuesta: Yes\n",
      "--------------------------------------------------------------------------------\n",
      "üß† Pregunta: How does LangChain handle streaming responses?\n",
      "üí¨ Respuesta: Stream All Runnable objects implement a sync method called stream and an async variant called astream . These methods are designed to stream the final output in chunks, yielding each chunk as soon as it is available.\n",
      "--------------------------------------------------------------------------------\n",
      "üß† Pregunta: What are some production-ready deployment strategies for LangChain apps?\n",
      "üí¨ Respuesta: Deployment : Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Platform\n",
      "--------------------------------------------------------------------------------\n",
      "üß† Pregunta: How can LangChain be integrated with frameworks like FastAPI or Streamlit?\n",
      "üí¨ Respuesta: LangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it. If you're looking to get up and running quickly with chat models , vector stores , or other LangChain components from a specific provider, check out our growing list of integrations . API reference Head to the reference section for full documentation of all classes and methods in the LangChain Python packages. Ecosystem  LangSmith Architecture page. langchain-core : Base abstractions for chat models and other components. Integration packages (e.g. langchain-openai , langchain-anthropic , etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers. LangChain-community : Third-party integrations that are community maintained. Tutorials and the API Reference . However, these guides will help you quickly accomplish common tasks using chat models , vector stores , and other common LangChain components. Check out LangGraph-specific how-tos here . Conceptual guide Introductions to all the key parts of LangChain you'll need to know! Here you'll find high level explanations of all LangChain concepts. For a deeper dive into LangGraph concepts, check out this page . Integrations\n",
      "--------------------------------------------------------------------------------\n",
      "üß† Pregunta: What are common use cases of LangChain in enterprise settings?\n",
      "üí¨ Respuesta: get up and running quickly with chat models , vector stores , or other LangChain components from a specific provider\n",
      "--------------------------------------------------------------------------------\n",
      "üß† Pregunta: How does LangChain work with multi-modal models?\n",
      "üí¨ Respuesta: a consistent interface for working with chat models from different providers while offering additional features for monitoring, debugging, and optimizing the performance of applications that use LLMs\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for pregunta in preguntas:\n",
    "    print(\"üß† Pregunta:\", pregunta)\n",
    "    print(\"üí¨ Respuesta:\", answer_question(pregunta))\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "86d8af7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_similarity(response, context):\n",
    "    \"\"\"\n",
    "    Calcula la similitud sem√°ntica entre una respuesta generada y su contexto de origen.\n",
    "\n",
    "    Utiliza embeddings generados por el modelo de `sentence-transformers` y calcula la\n",
    "    similitud del coseno entre la respuesta y el contexto proporcionado. Esto permite\n",
    "    estimar qu√© tan fiel es la respuesta al contenido recuperado.\n",
    "\n",
    "    Args:\n",
    "        response (str): Texto generado por el modelo (respuesta).\n",
    "        context (str): Texto fuente usado como contexto en el prompting.\n",
    "\n",
    "    Returns:\n",
    "        float: Valor de similitud del coseno entre 0 y 1 (mayor valor = mayor similitud).\n",
    "    \"\"\"\n",
    "    response_embedding = embedding_model.encode([response])\n",
    "    context_embedding = embedding_model.encode([context])\n",
    "    similarity = cosine_similarity(response_embedding, context_embedding)\n",
    "    return similarity[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "974afc45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¢ Similitud respuesta-contexto: 0.32325405\n",
      "a framework for developing applications powered by large language models\n",
      "this page\n",
      ".\n",
      "Integrations\n",
      "‚Äã\n",
      "LangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\n",
      "If you're looking to get up and running quickly with\n",
      "chat models\n",
      ",\n",
      "vector stores\n",
      ",\n",
      "or other LangChain components from a specific provider, check out our growing list of\n",
      "integrations\n",
      ".\n",
      "API reference\n",
      "‚Äã\n",
      "Head to the reference section for full documentation of all classes and methods in the LangChain Python packages.\n",
      "Ecosystem\n",
      "‚Äã\n",
      "ü¶úüõ†Ô∏è LangSmith\n",
      "‚Äã\n",
      "\n",
      "here\n",
      ".\n",
      "Marked as deprecated a number of legacy chains and added migration guides for all of them. These are slated for removal in\n",
      "langchain\n",
      "1.0.0. See the deprecated chains and associated\n",
      "migration guides here\n",
      ".\n",
      "How to update your code\n",
      "‚Äã\n",
      "If you're using\n",
      "langchain\n",
      "/\n",
      "langchain-community\n",
      "/\n",
      "langchain-core\n",
      "0.0 or 0.1, we recommend that you first\n",
      "upgrade to 0.2\n",
      ".\n",
      "If you're using\n",
      "langgraph\n",
      ", upgrade to\n",
      "langgraph>=0.2.20,<0.3\n",
      ". This will work with either 0.2 or 0.3 versions of all the base packages.\n",
      "\n",
      "Introduction | ü¶úÔ∏èüîó LangChain\n",
      "Skip to main content\n",
      "Our\n",
      "Building Ambient Agents with LangGraph\n",
      "course is now available on LangChain Academy!\n",
      "On this page\n",
      "LangChain\n",
      "is a framework for developing applications powered by large language models (LLMs).\n",
      "LangChain simplifies every stage of the LLM application lifecycle:\n",
      "Development\n",
      ": Build your applications using LangChain's open-source\n",
      "components\n",
      "and\n",
      "third-party integrations\n",
      ".\n",
      "Use\n",
      "LangGraph\n"
     ]
    }
   ],
   "source": [
    "context = search_similar_chunks(\"What is LangChain?\")\n",
    "respuesta = answer_question(\"What is LangChain?\")\n",
    "print(\"üî¢ Similitud respuesta-contexto:\", score_similarity(respuesta, context))\n",
    "print(respuesta)\n",
    "print(context)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
