{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d44d3fb0",
   "metadata": {},
   "source": [
    "# Prueba T√©cnica ‚Äì Ingeniero/a de IA Aplicada: Sistemas RAG y LLMs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04c04b1",
   "metadata": {},
   "source": [
    "## Importar librer√≠as necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc02f605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b0777f",
   "metadata": {},
   "source": [
    "## 1. Data Ingestion "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f990a650",
   "metadata": {},
   "source": [
    "- Selecciona una base documental (puede ser p√∫blica; se sugiere documentaci√≥n t√©cnica o art√≠culos en PDF/HTML/TXT). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19843d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_url(url):\n",
    "    \"\"\"\n",
    "    Descarga una p√°gina web en formato HTML y extrae su contenido textual limpio.\n",
    "\n",
    "    Este m√©todo realiza una solicitud HTTP a la URL proporcionada, elimina elementos\n",
    "    no textuales del HTML (como scripts, estilos, navegaci√≥n, etc.), y retorna el\n",
    "    texto visible, limpio y estructurado en l√≠neas.\n",
    "\n",
    "    Args:\n",
    "        url (str): URL de la p√°gina web a procesar.\n",
    "\n",
    "    Returns:\n",
    "        str: Texto limpio extra√≠do del contenido HTML de la p√°gina.\n",
    "\n",
    "    Raises:\n",
    "        Exception: Si la solicitud HTTP devuelve un c√≥digo diferente a 200.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Error al descargar la p√°gina: {response.status_code}\")\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Eliminamos scripts, estilos, navegaci√≥n, etc.\n",
    "    for tag in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\", \"aside\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    # Extraemos el texto visible y limpiamos espacios\n",
    "    text = soup.get_text(separator=\"\\n\")\n",
    "    lines = [line.strip() for line in text.splitlines()]\n",
    "    clean_text = \"\\n\".join(line for line in lines if line)\n",
    "\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c1d890",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_texts(urls):\n",
    "    \"\"\"\n",
    "    Extrae y concatena el texto limpio de m√∫ltiples URLs en formato HTML.\n",
    "\n",
    "    Para cada URL en la lista, se descarga el contenido HTML, se limpia y se\n",
    "    extrae el texto visible utilizando la funci√≥n `extract_text_from_url`.\n",
    "    Si ocurre un error durante la descarga o procesamiento, se imprime el\n",
    "    error y se contin√∫a con las siguientes URLs.\n",
    "\n",
    "    Args:\n",
    "        urls (list of str): Lista de URLs a procesar.\n",
    "\n",
    "    Returns:\n",
    "        str: Texto combinado de todas las p√°ginas, separado por saltos de l√≠nea.\n",
    "    \"\"\"\n",
    "    all_text = \"\"\n",
    "    for url in urls:\n",
    "        try:\n",
    "            print(f\"üîó Procesando: {url}\")\n",
    "            all_text += extract_text_from_url(url) + \"\\n\"\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error en {url}: {e}\")\n",
    "    return all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f087f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Links de LangChain\n",
    "urls = [\n",
    "    \"https://python.langchain.com/docs/versions/v0_3/\",\n",
    "    \"https://python.langchain.com/docs/introduction/\",\n",
    "    \"https://python.langchain.com/docs/tutorials/\",\n",
    "    \"https://python.langchain.com/docs/tutorials/retrievers/\",\n",
    "    \"https://python.langchain.com/docs/concepts/document_loaders/\",\n",
    "    \"https://python.langchain.com/docs/concepts/embedding_models/\",\n",
    "    \"https://python.langchain.com/docs/concepts/vectorstores/\",\n",
    "    \"https://python.langchain.com/docs/tutorials/classification/\",\n",
    "    \"https://python.langchain.com/docs/concepts/structured_outputs/\", \n",
    "    \"https://python.langchain.com/docs/tutorials/extraction/\", \n",
    "    \"https://python.langchain.com/docs/how_to/\", \n",
    "    \"https://python.langchain.com/docs/how_to/structured_output/\",\n",
    "    \"https://python.langchain.com/docs/how_to/tool_calling/\", \n",
    "    \"https://python.langchain.com/docs/how_to/streaming/\", \n",
    "    \"https://python.langchain.com/docs/how_to/debugging/\", \n",
    "    \"https://python.langchain.com/docs/concepts/few_shot_prompting/\", \n",
    "    \"https://python.langchain.com/docs/concepts/chat_models/\",\n",
    "]\n",
    "\n",
    "raw_text = extract_all_texts(urls)\n",
    "print(raw_text[:1000]) # Muestra los primeros 1000 caracteres del texto extra√≠do"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ae216f",
   "metadata": {},
   "source": [
    "- Procesa e indexa la informaci√≥n para facilitar la recuperaci√≥n sem√°ntica. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d883f0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(raw_text)\n",
    "\n",
    "print(f\"Total de chunks generados: {len(chunks)}\")\n",
    "print(chunks[0])  # Muestra el primer chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743f190e",
   "metadata": {},
   "source": [
    "## 2. Retrieval Pipeline (RAG) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb5e94f",
   "metadata": {},
   "source": [
    "- Implementa un motor de b√∫squeda sem√°ntica (p. ej., con FAISS o Elasticsearch). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31551826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo compacto y eficiente\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Crea los embeddings\n",
    "embeddings = embedding_model.encode(chunks, show_progress_bar=True)\n",
    "\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a0c602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el √≠ndice FAISS\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(np.array(embeddings))\n",
    "\n",
    "# Asociamos los chunks con sus √≠ndices (opcional, √∫til para trazabilidad)\n",
    "chunk_map = {i: chunk for i, chunk in enumerate(chunks)}\n",
    "\n",
    "print(f\"Total de vectores indexados: {index.ntotal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a73722e",
   "metadata": {},
   "source": [
    "- Realiza  embedding  de  los  documentos  usando  un  modelo  como  sentence-\n",
    "transformers o equivalente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6bff12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_similar_chunks(question, top_k=3):\n",
    "    \"\"\"\n",
    "    Recupera los chunks m√°s relevantes del corpus en funci√≥n de una pregunta dada.\n",
    "\n",
    "    Genera el embedding de la pregunta, lo compara con los vectores indexados en FAISS\n",
    "    y devuelve los `top_k` chunks m√°s similares. Los resultados se concatenan como un solo\n",
    "    string separado por doble salto de l√≠nea.\n",
    "\n",
    "    Args:\n",
    "        question (str): Pregunta en lenguaje natural.\n",
    "        top_k (int, optional): N√∫mero de chunks m√°s relevantes a recuperar. Por defecto es 3.\n",
    "\n",
    "    Returns:\n",
    "        str: Texto combinado de los chunks m√°s similares, separados por saltos de l√≠nea dobles.\n",
    "    \"\"\"\n",
    "    query_embedding = embedding_model.encode([question])\n",
    "    distances, indices = index.search(np.array(query_embedding), top_k)\n",
    "    retrieved_chunks = [chunk_map[i] for i in indices[0]]\n",
    "    return \"\\n\\n\".join(retrieved_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f7d0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is LangChain and what is it used for?\"\n",
    "context = search_similar_chunks(question, top_k=5)\n",
    "print(\"üîç Contexto recuperado:\\n\")\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465adc45",
   "metadata": {},
   "source": [
    "## 3. Generaci√≥n con LLM "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7258fd75",
   "metadata": {},
   "source": [
    "- Integra un modelo de lenguaje como OpenAI, HuggingFace, o Llama.cpp.. \n",
    "- Aseg√∫rate de que las respuestas generadas est√©n condicionadas por los documentos recuperados (prompting controlado). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0046203d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/flan-t5-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Carga un pipeline de generaci√≥n\n",
    "llm_pipeline = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cff6fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(context, question):\n",
    "    \"\"\"\n",
    "    Construye un prompt instructivo para un modelo de lenguaje, usando contexto y pregunta.\n",
    "\n",
    "    El prompt resultante indica expl√≠citamente al modelo que debe generar una respuesta\n",
    "    basada √∫nicamente en el contexto proporcionado, lo cual es esencial en sistemas RAG\n",
    "    para reducir la alucinaci√≥n y aumentar la fidelidad de la respuesta.\n",
    "\n",
    "    Args:\n",
    "        context (str): Fragmento(s) de texto recuperado(s) desde el corpus (chunks relevantes).\n",
    "        question (str): Pregunta formulada por el usuario.\n",
    "\n",
    "    Returns:\n",
    "        str: Prompt formateado para ser procesado por un modelo generativo.\n",
    "    \"\"\"\n",
    "    return f\"\"\"Answer the following question based solely on the context provided.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c9f14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question):\n",
    "    \"\"\"\n",
    "    Genera una respuesta en lenguaje natural utilizando un modelo LLM y recuperaci√≥n sem√°ntica.\n",
    "\n",
    "    Esta funci√≥n realiza los siguientes pasos:\n",
    "    1. Recupera los chunks m√°s relevantes desde el √≠ndice sem√°ntico basado en la pregunta.\n",
    "    2. Construye un prompt controlado con dicho contexto y la pregunta original.\n",
    "    3. Env√≠a el prompt al modelo LLM para generar una respuesta.\n",
    "\n",
    "    Args:\n",
    "        question (str): Pregunta en lenguaje natural a responder.\n",
    "\n",
    "    Returns:\n",
    "        str: Respuesta generada por el modelo de lenguaje, basada en el contexto recuperado.\n",
    "    \"\"\"\n",
    "    context = search_similar_chunks(question, top_k=3)\n",
    "    prompt = build_prompt(context, question)\n",
    "    response = llm_pipeline(prompt, max_new_tokens=512, do_sample=False)[0][\"generated_text\"]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5656d5",
   "metadata": {},
   "source": [
    "## 4. Evaluaci√≥n b√°sica "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0546fa",
   "metadata": {},
   "source": [
    "- Proporciona ejemplos de preguntas sobre los documentos. \n",
    "- Incluye  m√©tricas  o criterios para verificar que  las respuestas  generadas sean \n",
    "relevantes y fieles al contenido. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045aaf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "preguntas = [\n",
    "    \"What is LangChain?\",\n",
    "    \"What are the main components of LangChain?\",\n",
    "    \"What are LLM models used for in LangChain?\",\n",
    "    \"Can LangChain be integrated with databases?\",\n",
    "    \"What are the advantages of using LangChain in production?\",\n",
    "    \"How does LangChain differ from direct use of OpenAI API?\",\n",
    "    \"What are agents in LangChain and how do they work?\",\n",
    "    \"Can LangChain interact with external APIs?\",\n",
    "    \"How does LangChain support memory in conversations?\",\n",
    "    \"What are tools in LangChain and how are they defined?\",\n",
    "    \"How can I integrate LangChain with vector databases like FAISS or Pinecone?\",\n",
    "    \"What types of memory does LangChain support?\",\n",
    "    \"How do you persist memory in a LangChain app?\",\n",
    "    \"What are retrieval-based QA chains in LangChain?\",\n",
    "    \"How does LangChain handle prompt templating?\",\n",
    "    \"What is the role of LangChain Expression Language (LCEL)?\",\n",
    "    \"How do you debug chains and agents in LangChain?\",\n",
    "    \"Can LangChain be used with Hugging Face models?\",\n",
    "    \"How does LangChain handle streaming responses?\",\n",
    "    \"What are some production-ready deployment strategies for LangChain apps?\",\n",
    "    \"How can LangChain be integrated with frameworks like FastAPI or Streamlit?\",\n",
    "    \"What are common use cases of LangChain in enterprise settings?\",\n",
    "    \"How does LangChain work with multi-modal models?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8da9e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pregunta in preguntas:\n",
    "    print(\"üß† Pregunta:\", pregunta)\n",
    "    print(\"üí¨ Respuesta:\", answer_question(pregunta))\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d8af7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_similarity(response, context):\n",
    "    \"\"\"\n",
    "    Calcula la similitud sem√°ntica entre una respuesta generada y su contexto de origen.\n",
    "\n",
    "    Utiliza embeddings generados por el modelo de `sentence-transformers` y calcula la\n",
    "    similitud del coseno entre la respuesta y el contexto proporcionado. Esto permite\n",
    "    estimar qu√© tan fiel es la respuesta al contenido recuperado.\n",
    "\n",
    "    Args:\n",
    "        response (str): Texto generado por el modelo (respuesta).\n",
    "        context (str): Texto fuente usado como contexto en el prompting.\n",
    "\n",
    "    Returns:\n",
    "        float: Valor de similitud del coseno entre 0 y 1 (mayor valor = mayor similitud).\n",
    "    \"\"\"\n",
    "    response_embedding = embedding_model.encode([response])\n",
    "    context_embedding = embedding_model.encode([context])\n",
    "    similarity = cosine_similarity(response_embedding, context_embedding)\n",
    "    return similarity[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974afc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = search_similar_chunks(\"What is LangChain?\")\n",
    "respuesta = answer_question(\"What is LangChain?\")\n",
    "print(\"üî¢ Similitud respuesta-contexto:\", score_similarity(respuesta, context))\n",
    "print(respuesta)\n",
    "print(context)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
